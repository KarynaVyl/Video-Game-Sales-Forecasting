# -*- coding: utf-8 -*-
"""Video games sales.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FAFo1HX7AwE2Z4iHMflrzLulA_n3lefu
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
sales = pd.read_csv("vgsales.csv", index_col=0)
print(sales.shape)
sales.head(100)

#Очистка данних

import pandas as pd

# Завантаження даних
sales = pd.read_csv("vgsales.csv", index_col=0)

# Огляд даних
print(sales.info())

# Видалення дублікатів
sales.drop_duplicates(inplace=True)

# Перевірка на відсутні значення
missing_values = sales.isnull().sum()
print(missing_values)

# Заповнення або видалення відсутніх значень
# Наприклад, видалення рядків з відсутніми значеннями у важливих колонках
sales.dropna(subset=['NA_Sales', 'EU_Sales', 'JP_Sales'], inplace=True)

# Якщо відсутні значення не критичні, можна заповнити їх середніми або медіанними значеннями
# sales['Column_Name'].fillna(sales['Column_Name'].mean(), inplace=True)

# Огляд даних після очистки
print(sales.info())
print(sales.head())

"""Висновок з результатів:
Розмірність даних: У наборі даних після очистки залишилося 16,597 записів (рядків) і 10 колонок. Було видалено 1 рядок, що ймовірно був дублікатом.

Відсутні значення:

Колонка Year: 270 пропущених значень. Це може вплинути на аналіз, оскільки рік випуску гри є  важливим фактором для прогнозування продажів. Варто розглянути додаткові методи для обробки цих пропусків (як заповнення медіанним значенням або розгляд інших змінних).

Колонка Publisher: 58 пропущених значень. Оскільки видавець також є важливим атрибутом для моделі, потрібно визначити, як обробити ці пропуски (заповнення або видалення відповідних рядків).

Атрибути:

Основні атрибути (Name, Platform, Genre, Publisher) представлені у вигляді текстових даних (тип object).
Продажі (NA_Sales, EU_Sales, JP_Sales, Other_Sales, Global_Sales) представлені у вигляді числових даних (тип float64).

Подальші дії:

Слід виконати додаткову обробку пропущених значень у колонках Year і Publisher.
Провести аналіз і кореляцію між продажами в різних регіонах (NA_Sales, EU_Sales, JP_Sales) та іншими атрибутами.
Також треба перетворити текстові змінні у числові, щоб підготувати дані для побудови моделі машинного навчання.

Загалом, дані добре підготовлені для подальшого аналізу та побудови прогнозної моделі.









"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# Завантаження даних
sales = pd.read_csv("vgsales.csv", index_col=0)

# Додаткова обробка пропущених значень
# Заповнення пропущених значень у колонці Year медіанним значенням
sales['Year'].fillna(sales['Year'].median(), inplace=True)

# Заповнення пропущених значень у колонці Publisher найпоширенішим значенням (модою)
sales['Publisher'].fillna(sales['Publisher'].mode()[0], inplace=True)

# Перетворення текстових змінних у числові за допомогою LabelEncoder
label_encoder = LabelEncoder()
sales['Platform'] = label_encoder.fit_transform(sales['Platform'])
sales['Genre'] = label_encoder.fit_transform(sales['Genre'])
sales['Publisher'] = label_encoder.fit_transform(sales['Publisher'])

# Аналіз кореляції між продажами в різних регіонах
correlation_matrix = sales[['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']].corr()
print(correlation_matrix)

# Візуалізація кореляційної матриці
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Кореляція між продажами в різних регіонах')
plt.show()

# Перегляд підготовлених даних перед побудовою моделі
print(sales.head())

"""1. Аналіз кореляції:
Кореляційна матриця показує, що продажі в Північній Америці (NA_Sales) і Європі (EU_Sales) мають високу кореляцію з глобальними продажами (Global_Sales).
Продажі в Японії (JP_Sales) мають помірну кореляцію з іншими регіонами. Це означає, що вони можуть додати нову інформацію для моделі.

2. Масштабування даних:
Оскільки значення продажів (NA_Sales, EU_Sales, JP_Sales, тощо) мають різні діапазони, можна застосувати масштабування, наприклад, за допомогою StandardScaler або MinMaxScaler.

3. Поділ на тренувальну та тестову вибірки:
Для побудови та оцінки моделі важливо розділити дані на тренувальну та тестову вибірки. Це дозволить перевірити, як добре модель буде працювати на невідомих даних.

4. Вибір та побудова моделі:
Залежно від вимог, можна спробувати кілька різних моделей, таких як Linear Regression, Decision Tree або Random Forest, і оцінити їх продуктивність.

"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Виділення ознак (features) та цільової змінної (target)
X = sales[['NA_Sales', 'EU_Sales', 'Other_Sales', 'Global_Sales']]
y = sales['JP_Sales']

# Поділ даних на тренувальну та тестову вибірки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Масштабування даних
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Побудова лінійної регресійної моделі
model = LinearRegression()
model.fit(X_train_scaled, y_train)

# Прогнозування на тестовій вибірці
y_pred = model.predict(X_test_scaled)

# Оцінка моделі
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

"""Отримане значення Mean Squared Error (MSE) 2.743709338877751e-05 є дуже малим. Це вказує на те, що лінійна регресійна модель добре прогнозує продажі в Японії на основі даних про продажі в інших регіонах.

Що це означає:
MSE вимірює середній квадрат різниці між прогнозованими та фактичними значеннями. Чим менше значення MSE, тим краще модель підходить до даних.

Малий MSE може означати, що модель добре навчилася на даних, але також важливо перевірити, чи немає переобучення (overfitting).
"""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
import numpy as np

# Використання крос-валідації для оцінки моделі
cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')

# Середній та стандартне відхилення MSE
mean_mse = np.mean(-cv_scores)
std_mse = np.std(-cv_scores)

print(f"Mean MSE from Cross-Validation: {mean_mse}")
print(f"Standard Deviation of MSE: {std_mse}")

import matplotlib.pyplot as plt
import seaborn as sns

# Обчислення залишків (різниця між фактичними та прогнозованими значеннями)
residuals = y_test - y_pred

# Побудова графіка залишків
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True, color='blue')
plt.title('Розподіл залишків')
plt.xlabel('Залишки')
plt.ylabel('Частота')
plt.show()

# Побудова графіка залишків проти прогнозованих значень
plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, color='blue', edgecolor='w', alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.title('Залишки проти прогнозованих значень')
plt.xlabel('Прогнозовані значення')
plt.ylabel('Залишки')
plt.show()

"""Модель демонструє прийнятний рівень точності з нормальним розподілом залишків, що свідчить про хорошу узгодженість прогнозованих та фактичних значень. Проте наявність деяких кластерів та піків у залишках може вказувати на те, що модель могла б бути покращена, можливо, шляхом додавання додаткових змінних або використання іншого алгоритму моделювання."""

import pandas as pd

# Завантаження даних з CSV файлу
df = pd.read_csv('vgsales.csv')

print(df.head())  # Показати перші кілька рядків
print(df.columns)  # Показати всі стовпці

df_encoded = pd.get_dummies(df, columns=['Platform', 'Genre', 'Publisher'])

#Спробуємо Gradient Boosting Regressor

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# Завантаження даних
df = pd.read_csv('vgsales.csv')

# Обробка категоріальних змінних
df_encoded = pd.get_dummies(df, columns=['Platform', 'Genre', 'Publisher'])

# Вибір ознак та мети
X = df_encoded[['NA_Sales', 'EU_Sales'] + [col for col in df_encoded.columns if col.startswith('Platform_') or col.startswith('Genre_') or col.startswith('Publisher_')]]
y = df_encoded['JP_Sales']

# Розділити дані на тренувальну та тестову вибірки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ініціалізація та навчання моделі
model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
model.fit(X_train, y_train)

# Прогнозування
y_pred = model.predict(X_test)

# Оцінка моделі
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# Додатково можна вивести важливість ознак
importances = model.feature_importances_
print("Feature Importances:")
for feature, importance in zip(X.columns, importances):
    print(f"{feature}: {importance}")

"""Результати виглядають обнадійливо. Ось ключові моменти:

Середня квадратична помилка (MSE): 0.0418
Важливість ознак:
NA_Sales: 32.35%
EU_Sales: 14.98%
Publisher_Nintendo: 23.27%
Publisher_Enix Corporation: 2.88%
Publisher_Sega: 0.70%
Publisher_Sony Computer Entertainment: 0.91%
Genre_Role-Playing: 7.70%
Genre_Platform: 0.51%
Genre_Racing: 0.10%
Genre_Sports: 0.19%
Genre_Action: 0.05%
Genre_Adventure: 0.02%
Genre_Fighting: 0.06%
Genre_Misc: 0.01%
Genre_Puzzle: 0.05%
Genre_Shooter: 0.38%
Genre_Simulation: 0.15%
Genre_Strategy: 0.01%

Деякі платформи та видавці мають дуже низьку важливість або взагалі не беруть участі в моделі. Це може вказувати на те, що вони мають менший вплив на прогнозування продажів у Японії.
"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Завантаження даних
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Створення моделі
model = xgb.XGBRegressor()

# Навчання моделі
model.fit(X_train, y_train)

# Оцінка моделі
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'MSE: {mse}')

"""MSE = 0.04397 є досить хорошим результатом. Це свідчить про те, що модель XGBoost має низьку середню квадратичну помилку, що вказує на точні прогнози.Модель XGBoost показала себе як ефективна у передбаченні продажів відеоігор в Японії. Це може бути ознакою того, що обрані ознаки (функції) є релевантними для моделювання."""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import pandas as pd

# Завантаження даних
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Створення моделі
model = xgb.XGBRegressor()

# Навчання моделі
model.fit(X_train, y_train)

# Прогнозування
y_pred = model.predict(X_test)

# Оцінка моделі
mse = mean_squared_error(y_test, y_pred)
print(f'MSE: {mse}')

# Створіть DataFrame для тестових даних
df_test = pd.DataFrame(X_test, columns=['Column1', 'Column2', 'Column3'])  # Замініть на правильні назви стовпців
df_test['Actual_Sales'] = y_test
df_test['Predicted_Sales'] = y_pred

# Відсортуйте ігри за прогнозованими продажами
sorted_df = df_test.sort_values(by='Predicted_Sales', ascending=False)

# Виберіть топ-10 ігор
top_games = sorted_df.head(10)

print(top_games[['Actual_Sales', 'Predicted_Sales']])  # Виведіть відповідні стовпці

print(X_test.columns)

import pandas as pd

# Припустимо, що 'df' містить стовпець 'Name' і всі інші дані
# Індекси в df і X_test можуть відрізнятися, тому потрібно їх з'єднати за допомогою індексів
X_test = X_test.reset_index()  # Скидає індекс, щоб потім з'єднати з df
df = df.reset_index()  # Скидає індекс в оригінальному датафреймі

# Об'єднання X_test з df, щоб додати стовпець 'Name'
result_df = pd.merge(X_test, df[['index', 'Name']], left_on='index', right_on='index', how='left')

# Додайте прогнозовані і фактичні продажі
result_df['Predicted_Sales'] = y_pred
result_df['Actual_Sales'] = y_test.values

# Виберіть лише необхідні стовпці
result_df = result_df[['Name', 'Actual_Sales', 'Predicted_Sales']]

# Відсортуйте результати за прогнозованими продажами
sorted_result_df = result_df.sort_values(by='Predicted_Sales', ascending=False)

# Виведіть топ-10 ігор з найвищими прогнозованими продажами
print(sorted_result_df.head(10))

# Виведіть топ-10 ігор з найнижчими прогнозованими продажами
print(sorted_result_df.tail(10))